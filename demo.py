# -*- coding: utf-8 -*-
"""DEMO.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EtmRgv2_DG7V3qA1dVgymb6SkaxR00F-

### DEMO with new data & the best model Random Forest
"""

## Load the libraries
import numpy as np
import pandas as pd

import random
random.seed(3)

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")

from sklearn.preprocessing import StandardScaler

from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.impute import SimpleImputer
from sklearn.impute import KNNImputer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, roc_auc_score, mean_absolute_error, mean_squared_error

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RepeatedStratifiedKFold

## Connect to the shared drive
from google.colab import drive
drive.mount('/content/drive')

# References:
# general tuning: https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide#:~:text=Some%20of%20the%20best%20hyperparameter,Hyperopt

# LR - https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/
# SVM - https://www.vebuso.com/2020/03/svm-hyperparameter-tuning-using-gridsearchcv/
# RF - https://www.kaggle.com/code/sociopath00/random-forest-using-gridsearchcv/notebook
# Decision Tree - https://www.kaggle.com/code/gauravduttakiit/hyperparameter-tuning-in-decision-trees
# XGBoost - https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning
# Naive Bayes - https://medium.com/analytics-vidhya/how-to-improve-naive-bayes-9fa698e14cba
# KNN - https://medium.datadriveninvestor.com/k-nearest-neighbors-in-python-hyperparameters-tuning-716734bc557f

## Model evaluation 
def evaluate_model(y_true,y_pred):
  cm = confusion_matrix(y_true, y_pred)
  sns.heatmap(cm, annot=True, fmt='d')
  plt.show()
  accuracy = accuracy_score(y_true, y_pred)
  print("Accuracy:", accuracy)
  auc = roc_auc_score(y_true, y_pred)
  print("AUC-ROC:", auc)
  precision = precision_score(y_true, y_pred)
  print("Precision:", precision)
  f1 = f1_score(y_true, y_pred)
  print("F1 Score:", f1)
  recall = recall_score(y_true, y_pred)
  print("Recall:", recall)

import pickle # save model
path = '/content/drive/Shareddrives/DATA245 Group#7/Final Project/Scripts/Saved Models/'

# load best model RF
# since the gridsearch process shows the default RF hyperparameters are the best set, no tuning was done for RF
# getting the RF base model for DEMO

best_model = pickle.load(open(path + 'RF_initial.pkl', 'rb'))

# use new data as testing data
newdata = pd.read_csv('/content/drive/Shareddrives/DATA245 Submission Materials/Python Scripts/demo/DEMO_data.csv')

newdata

print(newdata.SepsisLabel)

justdata = newdata.drop(columns=['SepsisLabel'])
justdata

best_model.predict(justdata) # 1-sepsis detected in 6hrs advanced, 0-sepsis not detected in 6hrs advanced